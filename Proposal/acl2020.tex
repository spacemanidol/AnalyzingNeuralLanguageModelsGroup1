\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{tabularx}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\comment}[1]{\textcolor{red}{\bf \small [#1]}}
\aclfinalcopy 
\setlength\titlebox{5cm}
\newcommand\BibTeX{B\textsc{ib}\TeX}
\usepackage{microtype}
\aclfinalcopy 
\setlength\titlebox{5cm}
\newcommand\BibTeX{B\textsc{ib}\TeX}
\title{Group 1 Proposal: Exploration in  Neural Language Model Treatment of Similarity between Idioms and Non-figurative Paraphrases}
\author{Daniel Campos \And
  Paige Finkelstein \And 
    Elena Khasanova  \And
  Wes Rose  \And
  Josh Tanner   }
\begin{document}
\maketitle
\section{Introduction}
Understanding words as more than just unique sequences of character codes has always been a core problem for machine learning in natural language processing. Fixed vector representations like Word2Vec \cite{mikolov2013distributed} and GloVe \cite{Pennington2014GloveGV} were at one point the standard solution to this problem, but have recently been largely replaced by contextualized word embeddings generated by neural language models such as  ELMo \cite{Peters2018DeepCW}, GPT \cite{Radford2018ImprovingLU} and BERT \cite{devlin2018bert}. Applying these word embeddings has provided state-of-the-art performance on a variety of NLP tasks, and there as been much work recently published that makes an attempt to understand their success. Many papers have attempted to investigate how these contextualized word embeddings, as well as earlier non-contextualized embeddings, encode various linguistic properties (see \citet{Belinkov_2019} for a detailed overview). We are interested in following in this vein of analysis to explore whether pre-trained language models are encoding semantic information about idiomatic language in their deeply contextualized embeddings, and whether this information can be used to identify similarity between an idiom and a non-figurative paraphrase.

\citet{bizzoni-lappin-2018-predicting} propose a new corpus for a metaphor paraphrasing task and use it to test a neural network on two classification problems, showing that their model produces results that are significantly correlated with human judgments. In addition to studies investigating how syntactic, semantic, and commonsense \citep{zhou2019evaluating} phenomenon are encoded in contextualized representations, \citet{shwartz2019pain} have presented an examination suite that includes 6 tasks related to lexical composition, and compare results on this suite for 6 different models, concluding that contextualized word representations typically perform better than static word embeddings on such tasks and that all of the models had more success on tasks related to \textit{detecting meaning shift} and more difficulty on \textit{recovering implicit meaning}. 

Our interest in idioms, as multi-word expressions whose meanings cannot simply be determined by the sum of their parts, is clearly related to investigations of lexical composition, and \citet{shwartz2019pain}'s framework that uses classification tasks to test the ability of textual representations to address lexical composition will be a useful model. We propose, however, to explore whether semantic information about idioms is being captured in deep contextualized embeddings by a task more similar to the metaphor paraphrasing classification task presented in \citet{bizzoni-lappin-2018-predicting} and also to compare these findings to an intrinsic evaluation of word embedding cosine similarity comparable to that found in \citet{Wang_2019} and \citet{van_Aken_2019}.




\section{Methods}

\subsection{Models}
We intend to build an analysis suite with Hugging Face's Transformer library\footnote{https://github.com/huggingface/transformers} and run the same experiments across multiple language models, though these may be limited to BERT and its other incarnations which accept SEP tokens, as our Idiom Paraphrase Probing Task requires classification of sentence pairs. Time allowing, we would also like to train a baseline system to compare our results against, though this is a stretch goal.

\subsection{Idiom Paraphrase Probing Task}

In order to determine whether BERT and other neural language models are encoding semantic information about the meaning behind common idioms in their contextualized word embeddings, we intend to train a lightweight classifier on top of BERT and other neural language models to identify paraphrases. Specifically, we plan to train the classifier to take two sentences as input and return a binary judgment as to whether or not the sentences can be considered paraphrases. We are interested in testing how the classifier performs on sentence pairs in which one sentence includes the correct usage of some idiom and the other sentence is a (semantically equivalent or nearly semantically equivalent) paraphrase using literal language, with our primary goal being to explore whether the semantic information encoded in contextual word embeddings is representative of the meaning of words even when they are used in idioms. 

The task of paraphrase detection has been widely researched in the field of NLP, using both supervised and unsupervised learning approaches, and the original BERT paper \citep{devlin2018bert} even mentions paraphrase detection specifically when announcing state-of-the-art task results obtained by fine tuning the BERT model.
Of particular inspiration for our purposes is recent work by \citet{bizzoni-lappin-2018-predicting}, who focus on the task of paraphrase detection with sentences containing metaphors specifically, and introduce a new dataset that incorporates a system of ranking paraphrases based on acceptability. Their work, however, is primarily concerned with proposing a novel deep neural network to perform the metaphor paraphrase classification task, while we are proposing to use the task to prob deep contextualized word embeddings for semantic understanding of idioms.

We expect that the actual training component of this task should be fairly straightforward as we intend to follow the trend of keeping the probing classifier as simple as possible in order for the success (or failure) of the model to be explained by the contextualized word embedding inputs. We've found \citet{shwartz2019pain}'s minimal ``Embed-Encode-Predict" approach to classification models used in 6 probing tasks related to lexical composition to be a particularly useful model, and we intend to follow the example in their research and other probing task papers of using accuracy and F1 scores as a means of evaluation for our probing model task.

We expect that a larger portion of our effort will be required for experimental, especially dataset, design and generation than for building and training the probing classifier. However, one point that we intend to be especially attentive towards is the model's performance on idioms that it has seen in training data versus those that it has seen for the first time during testing, and whether a discrepancy emerges between these two categories.

Finally, there is the concern of the suitability of using contextualized language embeddings as input for a simple classifier to attempt paraphrase detection and other semantic similarity tasks at all. However, as the models we are planning to use to generate the word embeddings boast state-of-the-art performance on GLUE tasks such as paraphrase detection and semantic similarity detection with fine tuning \citep{devlin2018bert}, we are expectant that they capable of this task in some capacity. If feasible, we are also interested in seeing how diagnostic classifiers trained on top of versions of BERT that have been fined tuned for these related GLUE tasks perform. 



\subsection{Comparison with Intrinsic Vector Similarity}

One popular method for performing intrinsic evaluation of static word embeddings has been to examine the cosine similarity between vectors, especially in comparison with human judgments. In recent years, this method of analysis has been rightfully questioned as a means of evaluation. \citet{faruqui-etal-2016-problems} offer a particularly detailed explanation of problems with this approach, including the fact that \textit{relatedness} is often conflated with \textit{similarity}, that word vectors often capture task-specific (rather than semantic) similarity, and the fact that there hasn't been compelling evidence of correlation between the performance of word vectors on intrinsic similarity and extrinsic tasks such as paraphrasing or entailment.

Despite such criticism, however, evaluation of vector similarity continues to be used as a method to investigate not only static global embeddings but also contextualized word embeddings produced by neural language models (e.g. \citet{van_Aken_2019}). \citet{huang_cho_bowman_2020} recently used vector representations of words including their surrounding contexts to attempt to automatically identify word senses from unsupervised data. \citet{ethayarajh2019contextual} measures cosine similarity between vectors in varying contexts at each layer of neural language models to conclude that  ``much like how upper layers of LSTMs produce more task-specific
representations (Liu et al., 2019a), upper layers of contextualizing models produce more context-specific representations" (56).

 We propose using a measure of word embedding cosine similarity as a supplemental means of evaluation to accompany our idiom paraphrase extrinsic task. We hypothesize that cosine similarity between an idiom and a non-figurative paraphrase (and words used in idioms and their corresponding literal paraphrase counterparts) will be correlated with how well the linear classification model performs on the paraphrase pair.
 
Specifically, we will compare the cosine similarity of word embeddings generated from words used in the context of idioms with those generated from their uses in literal contexts as well as with a non-figurative paraphrase. For example, for the idiom \textit{let the cat out of the bag}, which can be paraphrased as \textit{to allow a secret to be revealed}, ``cat" can be understood figuratively as ``secret." To explore whether the word embedding for ``cat" in the context of the idiom seems to be capturing similar information for the word embedding(s) for ``secret" in its general word sense, we will do the following:
 
 We will take a set of (at least 10) sentences that use the word ``cat."  Only one of these sentences will contain the idiom usage of \textit{let the cat out of the bag}, and all of the others will be the most common literal sense of ``cat." We will also take a set of (at least 10) sentences that contain the word ``secret," in its dominant literal usage (that corresponds to its usage in the paraphrase \textit{to allow a secret to be revealed}). 
 
 Next, we will use the HuggingFace Transformers library \citep{wolf2019huggingfaces} to tokenize, prepare, and feed each of the sentences as input into the pre-trained BERT model and to extract word embeddings for the indexes that correspond to ``cat" and ``secret" in each sentence respectively. Per \citet{ethayarajh2019contextual}'s findings that upper layers of neural language models tend to produce more context specific embeddings, we will likely take only the embedding from the last layer or an average across the embeddings from the last few layers.
 
 We will then perform pairwise comparisons of the cosine similarity between the various usages. We will calculate the average cosine similarity between all of the literal-context usages of ``cat" and every word-embedding for ``secret", and compare this with the averaged cosine similarity score for the figurative use of ``cat" with all of the  word embeddings representing ``secret." We will also compare the average cosine similarity between all of the literal usage word embeddings for ``cat" with the average cosine similarity for the idiom usage of ``cat" and each literal usage.
 
 Finally, we will also calculate the cosine similarity between the full sentence embedding representing the sentence containing the idiom and a sentence that is a literal paraphrase. These sentence pairs will be the same ones used in our idiom paraphrase probing task, and we will use them (as well as our word-specific embedding investigations) to try to see if there seems to be a correlation between vector cosine similarity scores and the classification performance.

\subsection{Datasets}
Exploration of available datasets so far has not yielded anything precisely in line with what we are looking to use, and so we expect to have to primarily construct our own datasets. We anticipate that this will take a non-trivial amount of work, and we intend to be attentive to the issues raised in \citet{niven2019probing} and \citet{mccoy2019right}, which show how unintentionally biased datasets can lead to inflated performance.

\subsubsection{Dataset for Idiom Paraphrase Probing Task}
Realistically, our dataset is likely to be a mix of pre-existing data and examples we produce ourselves. We have not settled on a final plan for generation of our data, and we expect that figuring out how to generate the precise data that we want without introducing the opportunity for the biases discussed above to develop may represent the most substantial challenge in this experiment. That said, we are including a rough outline for how this process might look. 

\begin{enumerate}
    \item Produce a list of candidate idioms. We will first need a dataset of idioms; Wiktionary\footnote{https://en.wiktionary.org/wiki/Category:English\_idioms} has a list of more than 8,000 idioms and there are several other datasets available, so it is likely just a matter of choosing the correct one. Then we will need to filter down to a subset of idioms we intend to use for our experiment, which we expect to be an at least partly manual process.
    \item Mine sentences including these idioms from a larger corpus. One strong candidate is the Reddit corpus,\footnote{https://files.pushshift.io/reddit/comments/} though again there may be multiple options for this. Once again, we will need to filter these at least partially manually down to a subset that are good candidates for being paraphrased, avoiding sentences which rely too heavily on their context and ideally keeping length relatively uniform.
    \item For each of the above sentences including idioms, manually produce at least one valid paraphrase that has minimal lexical overlap with the sentence including an idiom. 
    \item Time allowing, use primarily automated approaches to extend the variety of our data. Some possible approaches include:
    \begin{itemize}
        \item Generating invalid but lexically similar paraphrases by replacing one word of an idiom  with a semantically similar word such as transforming ``let the cat out of the bag" to ``let the kitten out of the bag" or by replacing one word with a distributionally likely word that produces a completely different literal meaning such as transforming ``have a broken heart" to ``have a broken leg."
        \item Generating varieties of our valid paraphrases by automatically replacing words in them with synonyms or acceptable hypernyms. 
    \end{itemize}

\end{enumerate}

We may also be able to reuse existing datasets (such as the data used in \citet{bizzoni-lappin-2018-predicting}) or combinations of existing datasets. In an ideal world, we would be able to find some way to meaningfully reuse parts of the relevant GLUE tasks. 

\subsubsection{Dataset for Vector Similarity Comparison}
As discussed in 2.3 above, we also have a specific dataset in mind that will overlap some with, but not be identical to the data used in our probing task. We anticipate having to generate this dataset ourselves and will make it public. We will start by identifying at least 50 commonly used English idioms and use each in a sentence. This idiom-containing-sentence and a paraphrase of it will be part of the data we include in the probing task. Then we will also need to generate a set of sentences that use a key word from the idiom in a literal manner, and a set of sentences the contain the corresponding paraphrased key word in typical usage. We will likely use a large free online corpus, such as the British National Corpus or the Corpus of Contemporary American English to help generate these sentences lists.





\section{Possible Results}

\subsection{Idiom Paraphrase Probing Task}
One possible result is that we will find that the classifier performs well on our task (i.e. high accuracy and F1 scores). If this is the case, we believe that it is evidence that something about idiom usage is captured in contextualized word embeddings. However, even with carefully constructing our datasets to prevent unintentional heuristics related to performance as we plan to do, it is unlikely that we will be able to conclude that what is captured is \textbf{necessarily} semantic meaning of words used idiomatically. 

Within this context of overall success, one potentially informative outcome will be how the model performs on sentence pairs that are comparisons where an idiomatically used word is replaced in one sentence with a word that is a synonym of its literal meaning but not its meaning in the context of the idiom, such as ``She let the cat out of the bag" and ``She let the kitten out of the bag," or  ``She finally decided to bite the bullet" and ``She eventually chewed the bullet." If the model is able to correctly predict that these sentences are not paraphrases, while also having high accuracy for predicting pairs that are paraphrases and not having seen this idiom during training, then we think this could be interpreted as evidence that the contextualized word embeddings for words used in idioms contain some semantic information particular to their idiomatic usage.

If, however, we observe that the model is effective at predicting paraphrases but only for sentences containing idioms that it has seen in the training data and fails at predicting paraphrases for sentence pairs containing idioms it has only seen at test time, then we will take this as evidence that semantic knowledge about idiom meaning is \textbf{not} being captured in the deep contextual word embeddings, at least not in a way that can be accessed and used for paraphrase detection in a comparable way to sentences that do not contain idioms.


\subsection{Vector Similarity Comparison}
One possible outcome is that when we compare the average cosine similarity between the word embedding for the usage of the word used figuratively in the context of an idiom with the similar paraphrased word (e.g. ``cat" in the context of ``let the cat out of the bag" with a paraphrase like ``secret")  it will be substantially higher than the average cosine similarity for the word in its common literal use (e.g. ``cat" in sentences such as ``the cat was on the roof"). If this is the case, we believe it is evidence that the word embeddings are capturing some semantic information about the idiomatic meaning of the word in context. 

Secondly, if we do observe such a pattern of higher cosine similarity between figurative word use in idioms and their paraphrase correlates, then we may be able to draw some further conclusions based on comparison with the results from our probing task. Specifically, if we find that idiom/paraphrase pairs where the cosine similarity is higher also tend to be predicted more accurately in the classification task, we believe this correlation would be evidence that vector cosine similarity in this context is a potentially useful intrinsic proxy for predicting downstream performance or, at least, that it suggests that the relationship between vector cosine similarity and paraphrase task performance deserves deeper analysis. 

If, on the other hand, we observe no meaningful increase in cosine similarity between words used in idioms over their literal usages when each is compared with the paraphrased word---but the idiom paraphrase probing task has high enough accuracy to suggest that something about idiom usage is being captured in these word embeddings---then we will take this as evidence that vector cosine similarity is not a useful measure of contextual semantic correlation.


\section{Division of labor + Timeline}
Our goal is to have a solid concept of our experiment as well as a robust dataset finalized by 2/20 so that we will have at least two full weeks to run the experiments, perform analysis, and write our paper with results. In the table below is our detailed plan for timeline and division of labor.

\clearpage

\begin{tabularx}{1.01\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X | }
 \hline
 \textbf{Task} & \textbf{Description} & \textbf{Due Date}  & \textbf{People} \\
 \hline
 Experimental design  &  Concretely design our experiments, and decide high level aspects of datasets
 & {first draft 2/6 \newline
 tentative final draft 2/13} & Josh, Paige, Elena \\
  \hline
 Detailed dataset design  & Decide how to generate our data (write vs fetch \newline \& filter) and what linguistic rules to apply generating 
 it
  & outline by 2/13 & Elena, Daniel, Josh \\
      \hline
 Data discovery\// processing\// generation  & Produce\//generate data according to dataset design
  & tentative final dataset by 2/20 & Elena, Daniel \\
  
      \hline
Experiment implementation\// running & Write code to read in data, train the diagnostic classifier, do vector space similarity measures for word embeddings, and produce accuracy results (potentially for multiple different base NNs)

  &  implementation by 2/20 \newline \newline
  training, testing, and accuracy results by 2/23 & Josh, Daniel, Paige \\
  
   \hline
 Data analysis & Analyze results from experiment
  &  2/27 & Paige, Wes \\
    
   \hline
 Paper writing & Write paper, including results and analysis
  &  3/5 & Paige, Wes \\
   \hline
Special Topic Presentation & Speaker
  &  2/20 & Wes, Daniel \\
     \hline
Colloquium Presentation
 & Speaker
  &  3/12 & Elena + ?? \\
  
  

\hline
\end{tabularx}
\clearpage
\bibliography{anthology,acl2020}
\bibliographystyle{acl_natbib}
\end{document}