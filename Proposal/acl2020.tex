\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{tabularx}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\newcommand{\comment}[1]{\textcolor{red}{\bf \small [#1]}}

\aclfinalcopy 
\setlength\titlebox{5cm}

\newcommand\BibTeX{B\textsc{ib}\TeX}
\usepackage{microtype}
\aclfinalcopy 
\setlength\titlebox{5cm}
\newcommand\BibTeX{B\textsc{ib}\TeX}
\title{Group 1 Proposal: Exploration in  Neural Language Model Treatment of Similarity between Idioms and Non-figurative Paraphrases}
\author{Daniel Campos \And
  Paige Finkelstein \And 
    Elena Khasanova  \And
  Wes Rose  \And
  Josh Tanner   }

\begin{document}
\maketitle
\section{Introduction}
Understanding words as more than just unique sequences of character codes has always been a core problem for machine learning in natural language processing. Fixed vector representations like Word2Vec \cite{Mikolov2013DistributedRO} and Glove \cite{Pennington2014GloveGV} were the standard solution to this problem for a long time, but have recently been largely replaced by contextualized word embeddings generated by neural language models such as  ELMO \cite{Peters2018DeepCW}, GPT \cite{Radford2018ImprovingLU} and BERT \cite{Devlin2019BERTPO}. Applying these word embeddings has provided state-of-the-art performance on a variety of NLP tasks, and there as been much work recently published that makes an attempt to understand their success. 
\comment{There is a point of view that says including more concrete examples of what other studies did and citing those papers would improve the introduction. There is another point of view that says that says that in the context of this class and even probably most conferences, this entire introduction is unnecessary - no one who doesn't know this stuff is going to be reading our paper. Things to think about.}

Many papers have attempted to investigate how these contextualized word embeddings, as well as earlier non-contextualized embeddings, encode various linguistic properties (see \citet{Belinkov_2019} for a detailed overview). We are interested in following in this vein of analysis to explore whether pre-trained language models are encoding semantic information about idiomatic language in their deeply contextualized embeddings, and whether this information can be used to identify similarity between an idiom and a non-figurative paraphrase.
\comment{@Spacemanidol - I dropped some of your concrete references to other papers here in terms of our proposed methodology. I think these would be very appropriate in our final paper, but we haven't even fixed our methodology yet.}

\citet{bizzoni-lappin-2018-predicting} propose a new corpus for a metaphor paraphrasing task and use it to test a neural network on two classification problems, showing that their model produces results that are significantly correlated with human judgments. In addition to studies investigating how syntactic, semantic, and commonsense \citep{zhou2019evaluating} phenomenon are encoded in contextualized representations, \citet{shwartz2019pain} have presented an examination suite that includes 6 tasks related to lexical composition, and compare results on this suite for 6 different models, concluding that contextualized word representations typically perform better than static word embeddings on such tasks and that all of the models had more success on tasks related to \textit{detecting meaning shift} and more difficulty on \textit{recovering implicit meaning}. 

Our interest in idioms, as multi-word expressions whose meanings cannot simply be determined by the sum of their parts, is clearly related to investigations of lexical composition, and \citet{shwartz2019pain}'s framework that uses classification tasks to test the ability of textual representations to address lexical composition will be a useful model. We propose, however, to explore whether semantic information about idioms is being captured in deep contextualized embeddings by a task more similar to the metaphor paraphrasing classification task presented in \citet{bizzoni-lappin-2018-predicting} and also to compare these findings to an intrinsic evaluation of word similarity similar to that found in \citet{Wang_2019} and \citet{van_Aken_2019}.




\section{Methods}

\subsection{Model}
We intend to build an analysis suite with Hugging Face's Transformer\footnote{https://github.com/huggingface/transformers} and run the same experiments across multiple language models, though these may be limited to BERT and its other incarnations (AlBERT, RoBERTa, etc.) as our Idiom Paraphrase Probing Task requires classification of sentence pairs. Time allowing, we would also like to train a baseline to compare to, though this is likely a stretch goal.

\subsection{Idiom Paraphrase Probing Task}

Some details of this task may depend on data availability and dataset choices, but broadly our intent is to train a simple probing classifier on the task of classifying two input sentences as synonymous or not. In particular, we will be looking at pairs of sentences that are paraphrases where one is literal and the other expresses its meaning via idiom or metaphor. Our approach will mirror the approach of \cite{bizzoni-lappin-2018-predicting} fairly closely, with our primary goal being to determine whether the semantic information encoded in contextual word embeddings is representative of the meaning of words even when they are used in idioms. 

\comment{I included metaphors here; we can drop that if necessary. This section also needs expansion and citation of other relevant work, ideally.}

\vspace{5mm}
[insert description here!]

\vspace{15mm}



\subsection{Comparison with Intrinsic Vector Similarity}

One popular method for performing intrinsic evaluation of static word embeddings has been to examine the cosine similarity between vectors, especially in comparison with human judgments. In recent years, this method of analysis has been rightfully questioned as a means of evaluation. \citet{faruqui-etal-2016-problems} offer a particularly detailed explanation of problems with this approach, including the fact that \textit{relatedness} is often conflated with \textit{similarity}, that word vectors often capture task-specific (rather than semantic) similarity, and the fact that there hasn't been compelling evidence of correlation between the performance of word vectors on intrinsic similarity and extrinsic tasks such as paraphrasing or entailment.

Despite such criticism, however, evaluation of vector similarity continues to be used as a method to investigate not only static global embeddings but also contextualized word embeddings produced by neural language models (e.g. \citet{van_Aken_2019}). \citet{huang_cho_bowman_2020} recently used vector representations of words including their surrounding contexts to attempt to automatically identify word senses from unsupervised data. \citep{ethayarajh2019contextual} measures cosine similarity between vectors in varying contexts at each layer of neural language models to conclude that `` ``much like how upper layers of LSTMs produce more task-specific
representations (Liu et al., 2019a), upper layers of contextualizing models produce more context-specific representations" (56).

 We propose using a measure of word embedding cosine similarity as a supplemental means of evaluation to accompany our idiom paraphrase extrinsic task. We hypothesize that cosine similarity between an idiom and a non-figurative paraphrase (and words used in idioms and their corresponding literal paraphrase counterparts) will be correlated with how well the linear classification model performs on the paraphrase pair.
 
 \subsubsection{Vector Similarity Comparison Method}
 
 We propose an experiment that will compare the cosine similarity of words used in the context of idioms with their uses in literal contexts as well as with a non-figurative paraphrase. For example, for the idiom \textit{let the cat out of the bag}, which can be paraphrased as \textit{to allow a secret to be revealed}, ``cat" can be understood figuratively as ``secret". To explore whether the word embedding for ``cat" in the context of the idiom seems to be capturing similar information for the word embedding(s) for ``secret" in its general word sense, we will do the following steps. 
 
 We will take a set of (at least 10) sentences that use the word ``cat".  Only one of these sentences will contain the idiom usage of \textit{let the cat out of the bag}, and all of the others will be the most common literal sense of ``cat." We will also take a set of (at least 10) sentences that contain the word ``secret," in its dominant literal usage (that corresponds to its usage in the paraphrase \textit{to allow a secret to be revealed}). 
 
 Next, we will use the HuggingFace Transformers library \citep{wolf2019huggingfaces} to tokenize, prepare, and feed each of the sentences as input into the pre-trained BERT model and to extract word embeddings for the indexes that correspond to ``cat" and ``secret" in each sentence respectively. Per \citep{ethayarajh2019contextual}'s findings that upper layers of neural language models tend to produce more context specific embeddings, we will likely take only the embedding from the last layer or an average across the embeddings from the last few layers.
 
 We will then perform pairwise comparisons of the cosine similarity between the various usages. We will calculate the average cosine similarity between all of the literal-context usages of ``cat" and every word-embedding for ``secret", and compare this with the averaged cosine similarity score for the figurative use of ``cat" with all of the  word embeddings representing ``secret." We will also compare the average cosine similarity between all of the literal usage word embeddings for ``cat" with the average cosine similarity for the idiom usage of ``cat" and each literal usage.
 
 Finally, we will also calculate the cosine similarity between the full sentence embedding representing the sentence containing the idiom and a sentence that is a literal paraphrase. These sentence pairs will be the same ones used in our idiom paraphrase probing task, and we will use them (as well as our word-specific embedding investigations) to try to see if there seems to be a correlation between vector cosine similarity scores and the classification performance.

\subsection{Datasets}
Exploration of available datasets so far has not yielded anything precisely in line with what we are looking to use, and so we expect to have to primarily construct our own datasets. We anticipate that this will take a non-trivial amount of work, and we intend to be attentive to the issues raised in \citep{niven2019probing} and \citep{mccoy2019right}, which show how unintentionally biased datasets can lead to inflated performance.

\subsection{Dataset for Idiom Paraphrase Probing Task}
\vspace{5mm}
Realistically, our dataset is likely to be a mix of pre-existing data and examples we produce ourselves. We have not settled on a final plan for generation of our data, and we expect that out how to generate the precise data that we want without introducing the opportunity for the biases discussed above to develop may represent the most substantial challenge in this experiment. That said, we have several candidates approaches we intend to explore.

First, there is the possibility of using an external source such as the Reddit corpus \footnote{https://files.pushshift.io/reddit/comments/} for sentences including idioms. We would also need a list of idioms for this; Wiktionary has more than 8,000 idioms available which we could attempt to filter down by frequency, or use a pre-filtered subset such as the one in \cite{Jochim2018SLIDEA}. This has the advantage of providing examples of idiom usages "in the wild", though we will still have to produce the literal text paraphrases and it may substantially bias the dataset toward certain idioms. 

\comment{I see no easy way to automate producing correct paraphrases for idioms; solutions like Word2Vec will by definition give vectors representing the most common usage of the word, which has typically has little to no bearing on its meaning as part of an idiom.}

We may also be able to reuse existing datasets (such as the data used in \cite{bizzoni-lappin-2018-predicting}) or combinations of existing datasets. \comment{We need more examples of possible datasets}

\vspace{15mm}


\subsection{Dataset for Word Embedding Cosine Similarity Comparison}
As discussed in 2.3.1 above, we also have a specific dataset in mind that will overlap some with, but not be identical to the data used in our probing task. We anticipate having to generate this dataset ourselves and will make it public. We will start by identifying at least 50 commonly used English idioms and use each in a sentence. This idiom-containing-sentence and a paraphrase of it will be part of the data we include in the probing task. Then we will also need to generate a set of sentences that use a key word from the idiom in a literal manner, and a set of sentences the contain the corresponding paraphrased key word in typical usage. We will likely use a large free online corpus, such as the British National Corpus or the Corpus of Contemporary American English to help generate these sentences lists.

\subsection{Evaluation}

\subsection{Idiom Paraphrase Probing}
\comment{Compare classification accuracy to human annotators. As a stretch goal, potentially compare classification accuracy on idiom/literal sentences with classification accuracy on literal/literal sentences.}

\subsection{Vector Similarity}
\vspace{5mm}
[insert description here!]

\vspace{15mm}






\section{Possible Results}
\comment{arg1}
\vspace{5mm}
[insert description here!]

\vspace{15mm}





\section{Division of labor + Timeline}
Our goal is to have a solid concept of our experiment as well as a robust dataset finalized by 2/20 so that we will have at least two full weeks to run the experiments, perform analysis, and write our paper with results. In the table below is our detailed plan for timeline and division of labor.

\clearpage

\begin{tabularx}{1.01\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X | }
 \hline
 \textbf{Task} & \textbf{Description} & \textbf{Due Date}  & \textbf{People} \\
 \hline
 Experimental design  &  Concretely design our experiments, and decide high level aspects of datasets
 & {first draft 2/6 \newline
 tentative final draft 2/13} & Josh, Paige, Elena \\
  \hline
 Detailed dataset design  & Decide how to generate our data (write vs fetch \newline \& filter) and what linguistic rules to apply generating 
 it
  & outline by 2/13 & Elena, Daniel, Josh \\
      \hline
 Data discovery\// processing\// generation  & Produce\//generate data according to dataset design
  & tentative final dataset by 2/20 & Elena, Daniel \\
  
      \hline
Experiment implementation\// running & Write code to read in data, train the diagnostic classifier, do vector space similarity measures for word embeddings, and produce accuracy results (potentially for multiple different base NNs)

  &  implementation by 2/20 \newline \newline
  training, testing, and accuracy results by 2/23 & Josh, Daniel \\
  
   \hline
 Data analysis & Analyze results from experiment
  &  2/27 & Paige, Wes \\
    
   \hline
 Paper writing & Write paper, including results and analysis
  &  3/5 & Paige, Wes \\
   \hline
Special Topic Presentation & Speaker
  &  2/20 & Wes + ?? \\
     \hline
Colloquium Presentation
 & Speaker
  &  3/12 & Elena + ?? \\
  
  

\hline
\end{tabularx}

\clearpage

\bibliography{anthology,acl2020}
\bibliographystyle{acl_natbib}


\end{document}